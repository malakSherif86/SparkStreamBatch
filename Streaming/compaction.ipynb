{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d8720e-2272-4725-94d0-f40e0c344259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting\n",
      "INFO:__main__:Compacting data from /data/spark_project/streamingNew1 to /data/spark_project/streamingNew1_compacted\n",
      "INFO:__main__:Data read successfully\n",
      "INFO:__main__:Repartitioned the data into 2 partition(s)\n",
      "INFO:__main__:Compacted data written to /data/spark_project/streamingNew1_compacted\n",
      "INFO:__main__:Data compaction job completed successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting\")\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataCompactionJob\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Specify the input and output paths\n",
    "input_path = \"/data/spark_project/streamingNew1\"\n",
    "output_path = \"/data/spark_project/streamingNew1_compacted\"\n",
    "\n",
    "logger.info(f\"Compacting data from {input_path} to {output_path}\")\n",
    "\n",
    "# Check if the input path exists\n",
    "check_path_command = f\"hdfs dfs -test -d {input_path}\"\n",
    "if subprocess.run(check_path_command, shell=True).returncode != 0:\n",
    "    logger.error(f\"Input path {input_path} does not exist. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Read the data from the specified path\n",
    "df = spark.read.format(\"parquet\").load(input_path)\n",
    "\n",
    "logger.info(\"Data read successfully\")\n",
    "\n",
    "# Repartition the data to control the number of output files\n",
    "num_partitions = 2  \n",
    "compacted_df = df.repartition(num_partitions)\n",
    "\n",
    "logger.info(f\"Repartitioned the data into {num_partitions} partition(s)\")\n",
    "\n",
    "# Write the data back to the output path\n",
    "compacted_df.write.mode(\"overwrite\").format(\"parquet\").save(output_path)\n",
    "\n",
    "logger.info(f\"Compacted data written to {output_path}\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "logger.info(\"Data compaction job completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19872141-b9fa-474a-85ec-bd281f37d663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
